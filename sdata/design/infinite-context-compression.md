# AI 对话的无限上下文压缩

> 状态：**设计（v1.0）**
>
> 目标：通过自动压缩历史消息实现“无限”对话上下文，当接近模型令牌限制时使用分段迭代摘要技术。
>
> 本设计扩展了现有的 AI 后端（`server.py`）和前端（`Ai.qml`），以支持长对话而不触及模型上下文窗口限制。
>
> 主要技术：**分段迭代摘要**，结合模型感知的压缩阈值。

---

## 1. 需求

### 1.1 功能性

- **自动压缩触发**  
  监控对话历史的令牌计数；当使用量超过可配置的安全边际（例如模型最大上下文大小的 80%）时触发压缩。

- **模型感知配置**  
  每个模型定义可指定：
  - `maxContextTokens`：模型的硬限制（例如 GPT‑4‑Turbo 为 128 000）。
  - `summaryModel`：可选轻量模型用于生成摘要（可与主模型相同）。
  - `summaryPrompt`：模型特定的摘要提示词。
  - `safetyMargin`：开始压缩时占 `maxContextTokens` 的比例（默认 0.8）。
  - `maxSegmentSize`：单次调用中摘要的最大消息数（默认 5–10）。

- **分段迭代摘要**  
  长历史被分割为片段；摘要迭代生成：
  1. 摘要前 N 条消息 → 摘要 S₁。
  2. 将 S₁ 与后 N 条消息结合 → 摘要 S₂。
  3. 重复直到所有历史消息被压缩为单个摘要消息。

- **保留最近上下文**  
  最近的消息（可配置）永不压缩，确保模型始终看到即时对话。

- **透明的 UI 表示**  
  压缩的摘要消息以独特的视觉样式显示在聊天 UI 中，并可展开查看原始消息（如果存储了）。

- **用户控制**  
  提供配置选项以：
  - 启用/禁用压缩。
  - 选择压缩策略（保守/激进）。
  - 保留代码块、用户问题或其他重要内容。

### 1.2 非功能性

- **最小延迟影响**  
  压缩异步运行；用户可在摘要后台生成时继续输入。

- **向后兼容性**  
  未定义 `maxContextTokens` 的模型继续无压缩工作（依赖模型的原始窗口）。

- **本地优先操作**  
  所有令牌计数和压缩决策在本地进行；仅最终的摘要请求（如需要）调用外部 API。

- **可扩展架构**  
  新的压缩策略（例如基于向量的检索、关键词提取）可添加而不破坏现有实现。

---

## 2. 架构

### 2.1 新组件

```
ContextManager（QML 单例）
├── tokenEstimator     // 估算消息的令牌数量
├── compressionPlanner // 决定何时/压缩什么
├── summaryGenerator   // 调用 LLM 进行摘要
└── segmentProcessor   // 处理分段迭代摘要

AiModel 扩展属性
├── maxContextTokens   // 模型的硬上下文限制
├── summaryModel       // 用于摘要的模型（可选）
├── summaryPrompt      // 摘要提示词模板
├── safetyMargin       // 阈值比例（默认 0.8）
└── maxSegmentSize     // 每摘要步骤的消息数（默认 5）
```

### 2.2 数据流

1. **用户发送新消息**  
   → 前端将消息添加到 `messageIDs` / `messageByID`。

2. **API 请求前**（`Ai.makeRequest`）  
   → `ContextManager` 计算过滤后消息的总令牌数。

3. **阈值检查**  
   → 如果 `总令牌数 > maxContextTokens × safetyMargin`，触发压缩。

4. **分段摘要**  
   → 历史消息（排除最近的 `keepRecent` 条消息）被分割为 `maxSegmentSize` 大小的段。  
   → 使用段特定的提示词迭代调用 `summaryModel`（或当前模型）。  
   → 生成最终的摘要消息。

5. **上下文替换**  
   → 用摘要消息（标记为 `isSummary: true`）替换压缩的历史消息。  
   → 相应更新 `messageIDs` / `messageByID`。

6. **继续 API 调用**  
   → 使用摘要消息 + 最近未压缩的消息构建请求。

---

## 3. 算法

### 3.1 令牌估算

简单估算器（足以用于阈值检测）：
```javascript
function estimateTokens(message) {
    const content = message.rawContent || message.content || '';
    const english = content.replace(/[^\x00-\x7F]/g, '').length;
    const chinese = content.length - english;
    // 大约 1 token ≈ 4 英文字符，1 中文字符 ≈ 2 tokens
    return Math.ceil(english / 4 + chinese * 2 + 5);
}
```

更准确的估算器可后续添加（例如在后端集成 `tiktoken`）。

### 3.2 压缩触发

```javascript
function shouldCompress(messages, modelConfig) {
    if (!modelConfig.maxContextTokens) return false;
    const total = messages.reduce((sum, msg) => sum + estimateTokens(msg), 0);
    const threshold = modelConfig.maxContextTokens * (modelConfig.safetyMargin || 0.8);
    return total > threshold;
}
```

### 3.3 分段迭代摘要

**输入**：消息列表 `msgs`、`modelConfig`、`keepRecent`（要保留的最近消息数）

**步骤**：
1. 将 `msgs` 分割为：
   - `preserved = msgs.slice(-keepRecent)`（永不压缩）
   - `toCompress = msgs.slice(0, -keepRecent)`
2. 将 `toCompress` 按 `maxSegmentSize`（例如 5）大小分组。
3. 对每个段 `seg`（按时间顺序）：
   - 如果存在先前摘要，将其作为系统消息前置。
   - 使用适当的提示词调用 `summaryModel`（见 §4）。
   - 存储生成的摘要。
4. 最终摘要成为新消息，具有：
   - `role: "system"`
   - `content: "[压缩历史] <摘要文本>"`
   - `isSummary: true`
   - `originalMessageCount: toCompress.length`
   - `originalMessageIds: [...]`（可选，用于可能的展开）

**输出**：`[summaryMessage, ...preserved]`

---

## 4. 提示词设计

### 4.1 基础摘要提示词（用于第一段）

```markdown
您是一个专业的对话摘要助手。将以下对话压缩为简洁摘要，保留：

1. **核心问题与解决方案** – 用户的主要问题和 AI 的解决思路。
2. **关键决策点** – 重要的结论或选择。
3. **代码/配置变更** – 任何代码片段、配置修改或技术细节。
4. **待办事项** – 未完成的任务或下一步计划。

使用第三人称客观叙述。摘要长度控制在原文的 20% 左右。

对话内容：
{{CONVERSATION}}

请生成摘要：
```

### 4.2 迭代摘要提示词（用于后续段）

```markdown
您正在处理长对话的连续摘要任务。

现有摘要（覆盖前 {{PREV_COUNT}} 条消息）：
{{PREV_SUMMARY}}

新消息（{{NEW_COUNT}} 条）：
{{NEW_MESSAGES}}

将新消息整合到现有摘要中，更新摘要内容。注意：
1. 如果新消息与现有摘要内容相关，进行合并更新。
2. 如果新消息引入全新主题，在摘要中添加新段落。
3. 保持摘要的连贯性和完整性。

更新后的摘要：
```

### 4.3 模型特定提示词

模型可以通过 `summaryPrompt` 属性覆盖基础提示词。例如 Claude：

```markdown
作为 Claude，您擅长提炼长对话。请通过以下方式进行摘要：
1. 识别核心叙事线。
2. 提取关键技术决策。
3. 记录任何代码模式或架构选择。
4. 保留对话意图。

专注于语义压缩而非词汇压缩。

对话：
{{CONVERSATION}}

摘要：
```

### 4.4 高级结构化摘要提示词（用于复杂技术对话）

对于需要保留详细技术上下文的复杂对话，可使用以下结构化提示词：

```markdown
您的任务是创建一个全面、详细的整个对话摘要，捕捉无缝继续工作所需的所有基本信息，确保不丢失任何上下文。此摘要将用于压缩对话，同时保留关键的技术细节、决策和进展。

## 最近上下文分析
特别关注导致触发此摘要的最新代理命令和工具执行。包括：
- **最后代理命令**：刚刚执行了哪些具体操作/工具
- **工具结果**：最近工具调用的关键结果（如果很长则截断，但保留基本信息）
- **即时状态**：摘要前系统正在做什么
- **触发上下文**：什么导致令牌预算超限

## 分析过程
在提供最终摘要之前，将您的分析包裹在 `<analysis>` 标签中以系统组织思路：
1. **时间顺序回顾**：按时间顺序浏览对话，识别关键阶段和过渡
2. **意图映射**：提取所有显式和隐式的用户请求、目标和期望
3. **技术清单**：记录所有提到的技术概念、工具、框架和架构决策
4. **代码考古**：记录所有讨论或修改的文件、函数和代码模式
5. **进展评估**：评估已完成与待完成的工作
6. **上下文验证**：确保捕获所有继续工作所需的关键信息
7. **最近命令分析**：记录最近操作中的具体代理命令和工具结果

## 摘要结构
您的摘要必须按顺序包含以下部分，遵循以下确切格式：

<analysis>
[时间顺序回顾：浏览对话阶段：初始请求 → 探索 → 实现 → 调试 → 当前状态]
[意图映射：列出每个显式用户请求及其消息上下文]
[技术清单：记录所有提到的技术、模式和决策]
[代码考古：记录讨论的每个文件、函数和代码变更]
[进展评估：已完成与待完成的工作及其具体状态]
[上下文验证：验证所有继续上下文已捕获]
[最近命令分析：最后执行的代理命令、工具结果（长则截断）、摘要前的即时状态]
</analysis>

<summary>
1. 对话概览：
- 主要目标：[所有显式用户请求和总体目标，包含确切引用]
- 会话上下文：[对话流程和关键阶段的高级叙述]
- 用户意图演变：[用户在对话过程中需求或方向的变化]

2. 技术基础：
- [核心技术 1]：[版本/详情和用途]
- [框架/库 2]：[配置和使用上下文]
- [架构模式 3]：[实现方法和理由]
- [环境详情 4]：[设置细节和约束]

3. 代码库状态：
- [文件名称 1]：
  - 目的：[为什么此文件对项目重要]
  - 当前状态：[最近变更或修改的摘要]
  - 关键代码段：[重要函数/类及其简要解释]
  - 依赖关系：[此文件如何关联其他组件]
- [文件名称 2]：
  - 目的：[在项目中的角色]
  - 当前状态：[修改状态]
  - 关键代码段：[关键代码块]
- [根据需要添加更多文件]

4. 问题解决：
- 遇到的问题：[技术问题、错误或面临的挑战]
- 实施的解决方案：[如何解决问题及其理由]
- 调试上下文：[正在进行的故障排除努力或已知问题]
- 经验教训：[发现的重要见解或模式]

5. 进度跟踪：
- 已完成任务：[已成功实现的任务及其状态指示器]
- 部分完成工作：[进行中的任务及其当前完成状态]
- 已验证结果：[通过测试确认工作的功能或代码]

6. 活动工作状态：
- 当前焦点：[最近消息中正在处理的确切内容]
- 最近上下文：[最近几次对话交流的详细描述]
- 工作代码：[最近修改或讨论的代码片段]
- 即时上下文：[摘要前正在处理的特定问题或功能]

7. 最近操作：
- 最后代理命令：[摘要前刚刚执行的具体工具/操作，包含确切命令名称]
- 工具结果摘要：[最近工具执行的关键结果 - 长结果截断但保留基本信息]
- 摘要前状态：[令牌预算超限时代理正在做什么]
- 操作上下文：[为什么执行这些特定命令及其与用户目标的关系]

8. 继续计划：
- [待完成任务 1]：[详情和具体后续步骤，包含逐字引用]
- [待完成任务 2]：[要求和继续上下文]
- [优先级信息]：[哪些任务最紧急或逻辑上连续]
- [下一步行动]：[立即下一步，包含最近消息中的直接引用]
</summary>

## 质量指南
- **精确性**：包含确切的文件名、函数名、变量名和技术术语
- **完整性**：捕获继续工作所需的所有上下文，无需重新阅读完整对话
- **清晰性**：为需要精确接续对话的人编写
- **逐字准确性**：对任务规范和最近工作上下文使用直接引用
- **技术深度**：包含足够细节以理解复杂技术决策和代码模式
- **逻辑流程**：以逐步构建理解的方式呈现信息

此摘要应作为全面的交接文档，能够无缝继续所有活动工作流，同时保留原始对话的完整技术和上下文丰富性。
```

---

## 5. 配置

### 5.1 模型级配置（在 `AiModel` 中）

```qml
// 带有压缩设置的模型定义示例
"gpt-4-turbo": aiModelComponent.createObject(this, {
    "name": "GPT‑4 Turbo",
    "maxContextTokens": 128000,
    "summaryModel": "gpt-3.5-turbo",      // 可选轻量模型
    "summaryPrompt": "",                  // 为空时使用默认
    "safetyMargin": 0.75,
    "maxSegmentSize": 5,
    // … 现有属性（端点、模型等）
}),
```

### 5.2 用户级配置（在 `Config.qml` 中）

```qml
property JsonObject contextCompression: JsonObject {
    property bool enable: true
    property string strategy: "auto"           // "auto", "conservative", "aggressive"
    property bool preserveCodeBlocks: true
    property bool preserveUserQuestions: true
    property int keepRecentMessages: 10        // 始终保留最后 N 条消息不压缩
    property int minMessagesBeforeCompression: 20
}
```

### 5.3 后端配置（环境变量）

```
II_AI_COMPRESSION_ENABLE=1
II_AI_SUMMARY_MODEL=gpt-3.5-turbo          # 后备摘要模型
II_AI_SAFETY_MARGIN=0.8
```

---

## 6. 集成点

### 6.1 前端（`Ai.qml`）

1. **扩展 `AiModel` 组件** – 添加 §5.1 中列出的新属性。
2. **添加 `ContextManager` 单例** – 实现令牌估算、阈值检查和压缩编排。
3. **修改 `makeRequest()`** – 在构建请求数据前调用 `ContextManager.compressIfNeeded()`。
4. **UI 表示** – 在 `AiMessage.qml` 中检测 `isSummary`，并以不同的背景、图标和展开/收起切换渲染。

### 6.2 后端（`server.py`）

1. **添加压缩端点** – `POST /v1/summarize`，接受消息列表并返回摘要（可复用与 `stream_chat` 相同的提供商）。
2. **可选令牌计数** – 集成 `tiktoken` 进行准确令牌限制（可推迟到 v2）。
3. **存储原始消息** – 如果提供了 `originalMessageIds`，将映射存储在 SQLite 中供后续检索。

### 6.3 配置（`Config.qml`）

1. **添加用户压缩设置**，如 §5.2 所示。
2. **更新模型定义**，在 `Config.options.ai.extraModels` 中包含 `maxContextTokens` 等。

---

## 7. 实施路线图

### 第一阶段 – 基础（2–3 周）
- 使用压缩属性扩展 `AiModel`。
- 使用简单令牌估算器和阈值检测实现 `ContextManager`。
- 修改 `Ai.makeRequest` 以调用压缩逻辑（尚未实际摘要）。
- 添加摘要消息的 UI 占位符。

### 第二阶段 – 摘要生成（3–4 周）
- 实现 `summaryGenerator`，调用后端的新 `/v1/summarize` 端点。
- 构建分段迭代摘要算法。
- 与前端压缩流集成。
- 在聊天历史中存储摘要消息。

### 第三阶段 – 优化与用户控制（2 周）
- 添加用户压缩设置面板。
- 实现摘要消息的展开/收起。
- 添加压缩事件的视觉指示器。
- 性能优化（后台摘要、缓存）。

### 第四阶段 – 高级功能（未来）
- 使用 `tiktoken` 进行准确令牌计数。
- 基于向量的检索以更好地选择上下文。
- 多模型摘要策略（例如抽取式 + 生成式）。
- 质量评估与自动重试。

---

## 8. 风险与缓解措施

| 风险 | 缓解措施 |
|------|------------|
| **摘要质量不稳定** | 提供模型特定提示词；允许用户展开摘要并查看原始消息；实现质量检查启发式方法。 |
| **压缩移除重要细节** | 始终保留最近消息；提供保留代码块、用户问题等的选项；存储原始消息 ID 以供检索。 |
| **延迟增加** | 异步运行摘要；使用轻量模型进行摘要；缓存每个对话段的摘要。 |
| **令牌估算不准确** | 从简单估算器开始；后续在后端集成 `tiktoken`；使用保守的安全边际。 |
| **用户对压缩上下文困惑** | 清晰区分摘要消息的 UI；工具提示解释压缩；可选禁用按钮。 |

---

## 9. 开放问题

1. **原始消息存储在哪里？**  
   选项 A：仅保存在前端内存中（重启后丢失）。  
   选项 B：存储在后端 SQLite 中，引用摘要消息。  
   **建议**：从选项 A 开始；在第四阶段添加选项 B。

2. **如何处理函数调用/工具输出的摘要？**  
   将它们作为对话的一部分；视为带有特殊 `[Function call]` 前缀的常规消息。

3. **压缩是否应可手动触发？**  
   是 – 添加 `/compress` 命令，强制摘要除最后 `keepRecentMessages` 条消息外的所有内容。

4. **多模态消息（图像、文件）如何处理？**  
   对于 v1，在令牌计数和摘要中忽略它们（保持不压缩）。后续添加基于标题的摘要。

---

## 10. 成功指标

- **用户可见**：对话可超过模型原生令牌限制而不出错。
- **性能**：压缩为典型历史记录添加 < 2 s 延迟（P95）。
- **质量**：用户很少需要展开摘要以理解上下文连续性。
- **采用率**：> 80% 的用户尝试后保持压缩启用。

---

*设计版本：1.0*  
*最后更新：2026‑01‑26*  
*作者：AI 辅助设计（GitHub Copilot）*
